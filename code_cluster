import sqlite3
import networkx as nx
import math
import re
import json
import numpy as np
from collections import defaultdict
import time
from typing import Dict, List, Tuple, Set, Any

def create_schema_graph(db_file):
    """
    Extract an optimized schema graph from the database with efficient construction.
    
    Args:
        db_file: Path to the SQLite database file
        
    Returns:
        A tuple of (nx.DiGraph, dict) containing the graph and metadata for later use
    """
    start_time = time.time()
    
    # Create directed graph
    graph = nx.DiGraph()
    
    # Connect to database
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()
    
    # Execute a single query to get all tables instead of multiple queries
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [table[0] for table in cursor.fetchall()]
    
    # Pre-allocate data structures to avoid repeated dictionary lookups
    primary_keys = {}
    foreign_keys = {}
    all_columns = {}
    table_column_info = {}
    
    # First pass: Get all schema information in batch where possible
    for table in tables:
        # Get columns for this table
        cursor.execute(f"PRAGMA table_info({table});")
        columns = cursor.fetchall()
        table_column_info[table] = columns
        
        # Get foreign keys for this table
        cursor.execute(f"PRAGMA foreign_key_list({table});")
        foreign_keys[table] = cursor.fetchall()
    
    # Second pass: Build graph nodes efficiently
    for table in tables:
        # Add table node with all attributes at once
        graph.add_node(table, type='table', node_class='table')
        
        columns = table_column_info[table]
        primary_keys[table] = []
        all_columns[table] = []
        
        # Add all column nodes for this table at once
        for col in columns:
            col_name = col[1]
            col_id = f"{table}.{col_name}"
            is_pk = col[5] == 1
            
            # Add column node with all attributes
            graph.add_node(col_id, 
                          type='column',
                          node_class='column', 
                          data_type=col[2],
                          table=table,
                          column_name=col_name,
                          is_primary_key=is_pk,
                          not_null=col[3]==1)
            
            all_columns[table].append(col_id)
            
            if is_pk:
                primary_keys[table].append(col_id)
    
    # Third pass: Build all edges efficiently
    # Pre-compute edge lists for batch addition
    edges_to_add = []
    
    # Add table-column edges
    for table, columns in all_columns.items():
        for col_id in columns:
            edges_to_add.append((table, col_id, {'relationship_type': 'table_column', 
                                               'relationship': f"Column in {table}"}))
    
    # Add same-table edges efficiently
    for table, columns in all_columns.items():
        for i, col1 in enumerate(columns):
            for col2 in columns[i+1:]:
                rel = {'relationship_type': 'same_table', 'relationship': f"Columns in same table ({table})"}
                edges_to_add.append((col1, col2, rel))
                edges_to_add.append((col2, col1, rel))
    
    # Process foreign keys and add relevant edges
    fk_relationships = []
    for table, fk_list in foreign_keys.items():
        for fk in fk_list:
            fk_col = fk[3]          # Column in current table
            ref_table = fk[2]       # Referenced table
            ref_col = fk[4]         # Referenced column
            
            fk_col_id = f"{table}.{fk_col}"
            ref_col_id = f"{ref_table}.{ref_col}"
            
            # Primary-Foreign Key (Column-Column)
            edges_to_add.append((fk_col_id, ref_col_id, 
                               {'relationship_type': 'pk_fk_column', 
                                'relationship': f"Foreign key reference"}))
            
            # Foreign Key (Column-Table)
            edges_to_add.append((fk_col_id, ref_table, 
                               {'relationship_type': 'fk_table', 
                                'relationship': f"Foreign key to table"}))
            
            # Primary-Foreign Key (Table-Table)
            edges_to_add.append((table, ref_table, 
                               {'relationship_type': 'pk_fk_table', 
                                'relationship': f"Table foreign key relationship"}))
            
            # Store for later use
            fk_relationships.append((fk_col_id, ref_table, ref_col_id))
    
    # Add Primary Key (Column-Table) edges
    for table, pk_cols in primary_keys.items():
        for pk_col in pk_cols:
            edges_to_add.append((pk_col, table, 
                               {'relationship_type': 'pk_table', 
                                'relationship': f"Primary key of table"}))
    
    # Add all edges at once
    graph.add_edges_from(edges_to_add)
    
    conn.close()
    
    # Create metadata for efficient access later
    metadata = {
        'tables': tables,
        'primary_keys': primary_keys,
        'foreign_keys': fk_relationships,
        'all_columns': all_columns
    }
    
    # Print statistics and timing
    table_count = len([n for n, d in graph.nodes(data=True) if d.get('type') == 'table'])
    col_count = len([n for n, d in graph.nodes(data=True) if d.get('type') == 'column'])
    
    print(f"Schema graph created in {time.time() - start_time:.2f} seconds with:")
    print(f" - {table_count} table nodes")
    print(f" - {col_count} column nodes")
    print(f" - {graph.number_of_edges()} total edges")
    
    # Count edges by relationship type (optional)
    edge_counts = {}
    for _, _, data in graph.edges(data=True):
        rel_type = data.get('relationship_type', 'unknown')
        edge_counts[rel_type] = edge_counts.get(rel_type, 0) + 1
    
    for rel_type, count in edge_counts.items():
        print(f" - {count} {rel_type} edges")
    
    return graph, metadata


class OptimizedWeightedFlowPathFinder:
    """
    Optimized class for finding relevant paths in a database schema graph
    using the Weighted Flow Diffusion algorithm.
    """
    
    def __init__(self, gamma=0.02, max_iterations=30):
        """
        Initialize the optimized weighted flow path finder.
        
        Args:
            gamma: Parameter for Gaussian kernel weighting (default: 0.02)
            max_iterations: Maximum iterations for flow diffusion (default: 30)
        """
        self.gamma = gamma
        self.max_iterations = max_iterations
        self.cache = {}  # For memoization
    
    def _compute_query_relevance(self, graph, query, use_embeddings=False):
        """
        Compute query relevance scores for all nodes using optimized methods.
        
        Args:
            graph: The schema graph
            query: The query text
            use_embeddings: Whether to use vector embeddings for similarity
            
        Returns:
            Dictionary mapping nodes to their query relevance scores
        """
        # Cache key for memoization
        cache_key = f"relevance_{hash(query)}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        query_terms = set(re.findall(r'\b\w+\b', query.lower()))
        relevance_scores = {}
        
        # Optimize by scanning all nodes just once
        for node_id, attrs in graph.nodes(data=True):
            node_type = attrs.get('type', '')
            
            if node_type == 'table':
                # For table nodes, use table name for relevance
                node_terms = set(re.findall(r'\b\w+\b', node_id.lower()))
                relevance = len(query_terms.intersection(node_terms)) * 2  # Give tables higher weight
                
            elif node_type == 'column':
                # For column nodes, use column name
                table = attrs.get('table', '')
                column = attrs.get('column_name', '')
                
                # Combine both table and column terms
                node_terms = set(re.findall(r'\b\w+\b', column.lower()))
                table_terms = set(re.findall(r'\b\w+\b', table.lower()))
                
                # Weight column matches higher than table matches for columns
                col_overlap = len(query_terms.intersection(node_terms)) * 1.5
                table_overlap = len(query_terms.intersection(table_terms)) * 0.5
                
                relevance = col_overlap + table_overlap
            else:
                relevance = 0
            
            # Bonus for data type mentions in query
            if node_type == 'column' and attrs.get('data_type'):
                data_type = attrs.get('data_type', '').lower()
                if data_type in query.lower():
                    relevance += 0.5
            
            # Bonus for primary keys as they're often important for joins
            if attrs.get('is_primary_key', False):
                relevance += 0.3
                
            relevance_scores[node_id] = relevance
        
        # Cache the result
        self.cache[cache_key] = relevance_scores
        return relevance_scores
    
    def reweight_graph(self, graph, query):
        """
        Create a weighted version of the graph based on query relevance.
        
        Args:
            graph: The original schema graph
            query: The query text
            
        Returns:
            A weighted graph with edge weights adjusted for query
        """
        # Compute query relevance
        node_relevance = self._compute_query_relevance(graph, query)
        
        # Create a weighted copy of the graph
        weighted_graph = graph.copy()
        
        # Reweight all edges in one pass
        for source, target, data in graph.edges(data=True):
            rel_type = data.get('relationship_type', '')
            
            # Base weight - start with different weights for different relationship types
            base_weight = {
                'pk_fk_column': 2.0,  # Highest priority for foreign key relationships
                'pk_fk_table': 1.8,   # Table-level foreign key relationships
                'same_table': 1.5,    # Same table columns
                'table_column': 1.2,  # Table to column relationships
                'pk_table': 1.4,      # Primary key to table
                'fk_table': 1.3       # Foreign key to referenced table
            }.get(rel_type, 1.0)
            
            # Apply Gaussian kernel based on node attribute similarity
            source_relevance = node_relevance.get(source, 0)
            target_relevance = node_relevance.get(target, 0)
            
            # Compute attribute distance (based on query_relevance)
            # Higher distance = less similar
            dist = abs(source_relevance - target_relevance)
            
            # Apply Gaussian kernel: weight = base_weight * exp(-gamma * dist²)
            weight = base_weight * math.exp(-self.gamma * (dist ** 2))
            
            # Set the new edge weight
            weighted_graph[source][target]['weight'] = weight
        
        return weighted_graph
    
    def find_seed_nodes(self, graph, query, limit=3):
        """
        Find the best seed nodes to start flow diffusion from.
        
        Args:
            graph: The schema graph
            query: The query text
            limit: Maximum number of seed nodes to return
            
        Returns:
            List of (node_id, score) tuples for the best seed nodes
        """
        # Compute query relevance if not already cached
        node_relevance = self._compute_query_relevance(graph, query)
        
        # Find highest scoring nodes
        scored_nodes = [(node, score) for node, score in node_relevance.items() if score > 0]
        top_nodes = sorted(scored_nodes, key=lambda x: x[1], reverse=True)[:limit]
        
        return top_nodes
    
    def flow_diffusion(self, graph, seed_node, alpha=5.0):
        """
        Run the flow diffusion algorithm to find relevant nodes.
        
        Args:
            graph: The weighted graph representation
            seed_node: The starting node for diffusion
            alpha: Source mass multiplier
            
        Returns:
            Dictionary mapping nodes to their importance scores
        """
        # Initialize node embeddings and mass
        nodes = list(graph.nodes())
        x = {node_id: 0.0 for node_id in nodes}
        m = {node_id: 0.0 for node_id in nodes}
        
        # Set source mass
        if seed_node in m:
            m[seed_node] = alpha
        else:
            # Find best match if seed node not found
            best_match = None
            best_score = -1
            
            for node_id in nodes:
                if seed_node.lower() in node_id.lower():
                    # Use degree as a proxy for importance if exact match not found
                    score = graph.degree(node_id)
                    if score > best_score:
                        best_match = node_id
                        best_score = score
            
            if best_match:
                m[best_match] = alpha
            else:
                # Fall back to the first node if no match
                m[nodes[0]] = alpha
        
        # Set sink capacities (here using uniform capacity of 1.0)
        t = {node_id: 1.0 for node_id in nodes}
        
        # Run flow diffusion for specified iterations
        for _ in range(self.max_iterations):
            # Find nodes with excess mass
            overflow_nodes = [node for node, mass in m.items() if mass > t[node]]
            
            if not overflow_nodes:
                break
                
            # Pick a random overflow node
            i = overflow_nodes[0]  # Deterministic for better reproducibility
            
            # Calculate weighted degree
            w_i = sum(graph[i][j].get('weight', 1.0) for j in graph.neighbors(i))
            
            if w_i == 0:
                continue  # Skip isolated nodes
                
            # Update node embedding
            x[i] += (m[i] - t[i]) / w_i
            
            # Set excess mass to distribute
            excess = m[i] - t[i]
            m[i] = t[i]
            
            # Distribute excess mass to neighbors
            for j in graph.neighbors(i):
                # Find edge weight
                edge_weight = graph[i][j].get('weight', 1.0)
                
                # Update mass at neighbor
                m[j] += excess * edge_weight / w_i
        
        return x
    
    def extract_paths(self, graph, node_importance, metadata, limit=10, max_path_length=4):
        """
        Extract meaningful paths from the subgraph of important nodes.
        
        Args:
            graph: The graph representation
            node_importance: Dictionary mapping nodes to importance scores
            metadata: Database metadata from schema extraction
            limit: Maximum number of paths to return
            max_path_length: Maximum length of a path
            
        Returns:
            List of paths (as strings) sorted by relevance
        """
        # Filter nodes with positive importance
        active_nodes = {node: score for node, score in node_importance.items() if score > 0}
        
        if not active_nodes:
            return []
        
        # Sort nodes by importance score
        sorted_nodes = sorted(active_nodes.items(), key=lambda x: x[1], reverse=True)
        top_nodes = [node for node, _ in sorted_nodes[:min(10, len(sorted_nodes))]]
        
        paths = []
        path_scores = {}
        
        # Extract potential table-column-table-column paths
        for start_node in top_nodes:
            # Use breadth-first search to find paths
            visited = set([start_node])
            queue = [(start_node, [start_node], 0)]  # (node, path, length)
            
            while queue:
                current, path, length = queue.pop(0)
                
                # If we've reached another important node and path is complete
                if current != start_node and current in top_nodes and length >= 1:
                    # Check if the path makes sense (e.g., table->column->table or column->table->column)
                    if self._is_valid_path(path, graph):
                        path_str = '.'.join(path)
                        score = sum(active_nodes.get(node, 0) for node in path)
                        
                        paths.append(path_str)
                        path_scores[path_str] = score
                
                # Continue BFS if path is not too long
                if length < max_path_length:
                    for neighbor in graph.neighbors(current):
                        if neighbor not in visited:
                            # Only continue along edges with enough weight
                            if graph[current][neighbor].get('weight', 0) > 0.5:
                                visited.add(neighbor)
                                queue.append((neighbor, path + [neighbor], length + 1))
        
        # Sort paths by score and take top 'limit'
        sorted_paths = sorted(path_scores.items(), key=lambda x: x[1], reverse=True)
        top_paths = sorted_paths[:limit]
        
        # Return just the paths
        return [path for path, _ in top_paths]
    
    def _is_valid_path(self, path, graph):
        """
        Check if a path is valid for SQL generation.
        
        Args:
            path: List of node IDs representing a path
            graph: The graph
            
        Returns:
            Boolean indicating if the path is valid
        """
        if len(path) < 2:
            return False
        
        # Check alternating pattern of tables and columns
        node_types = []
        for node in path:
            node_type = graph.nodes[node].get('type', '')
            node_types.append(node_type)
        
        # Valid patterns include:
        # - table -> column -> table -> ...
        # - column -> table -> column -> ...
        valid = True
        for i in range(len(node_types) - 1):
            if node_types[i] == node_types[i + 1]:
                # Two tables or two columns in a row need to have a valid relationship
                if i < len(path) - 1:
                    rel_type = graph[path[i]][path[i + 1]].get('relationship_type', '')
                    if rel_type not in ['pk_fk_column', 'pk_fk_table', 'same_table']:
                        valid = False
                        break
        
        return valid
    
    def find_top_relevant_paths(self, graph, metadata, query, limit=10):
        """
        Main method to find relevant paths for a query.
        
        Args:
            graph: The schema graph
            metadata: Database metadata
            query: The query text
            limit: Maximum number of paths to return
            
        Returns:
            List of relevant paths
        """
        # Step 1: Reweight the graph based on query relevance
        weighted_graph = self.reweight_graph(graph, query)
        
        # Step 2: Find the best seed nodes
        seed_nodes = self.find_seed_nodes(weighted_graph, query)
        
        if not seed_nodes:
            return []
        
        # Step 3: Run flow diffusion from the best seed node
        seed_node, _ = seed_nodes[0]
        node_importance = self.flow_diffusion(weighted_graph, seed_node)
        
        # Step 4: Extract meaningful paths
        paths = self.extract_paths(weighted_graph, node_importance, metadata, limit)
        
        # If we didn't get enough paths, try with other seed nodes
        if len(paths) < limit and len(seed_nodes) > 1:
            for seed_node, _ in seed_nodes[1:]:
                if len(paths) >= limit:
                    break
                    
                # Run diffusion from this seed
                node_importance = self.flow_diffusion(weighted_graph, seed_node)
                
                # Extract more paths
                additional_paths = self.extract_paths(weighted_graph, node_importance, metadata, 
                                                     limit=limit-len(paths))
                
                # Add unique paths
                for path in additional_paths:
                    if path not in paths:
                        paths.append(path)
        
        return paths


class OptimizedSQLGenerator:
    """
    Optimized component for generating SQL for subqueries based on relevant paths.
    """
    
    def __init__(self, db_schema):
        """
        Initialize the SQL generator with database schema information.
        
        Args:
            db_schema: Dictionary containing database schema information
        """
        self.db_schema = db_schema
        self.cache = {}
    
    def generate_subsqls(self, paths, subquery, limit=5):
        """
        Generate SQL statements for a subquery based on relevant paths.
        
        Args:
            paths: List of paths relevant for this subquery
            subquery: The subquery text
            limit: Maximum number of SQL statements to generate
            
        Returns:
            List of SQL statements
        """
        if not paths:
            return []
        
        # Structure the paths to help with SQL generation
        structured_paths = self._structure_paths(paths)
        
        # Generate SQL candidates for each structured path
        sql_candidates = []
        
        for struct in structured_paths:
            table = struct.get('main_table')
            if not table:
                continue
                
            # Generate a basic SELECT query
            columns = struct.get('columns', [])
            joins = struct.get('joins', [])
            
            # Basic SELECT
            sql = f"SELECT {', '.join(columns)} FROM {table}"
            
            # Add JOINs
            for join in joins:
                join_table = join.get('table')
                join_condition = join.get('condition')
                if join_table and join_condition:
                    sql += f" JOIN {join_table} ON {join_condition}"
            
            # Add a basic WHERE clause based on the subquery
            where_clause = self._generate_where_clause(subquery, struct)
            if where_clause:
                sql += f" WHERE {where_clause}"
                
            sql_candidates.append(sql)
            
            # Generate variations with different clauses
            if limit > 1 and len(sql_candidates) < limit:
                # Version with GROUP BY if aggregation seems needed
                if any(word in subquery.lower() for word in ['count', 'sum', 'average', 'group', 'total']):
                    group_cols = [col for col in columns if not any(agg in col.lower() for agg in ['count(', 'sum(', 'avg('])]
                    if group_cols:
                        group_sql = sql + f" GROUP BY {', '.join(group_cols)}"
                        sql_candidates.append(group_sql)
                
                # Version with ORDER BY if sorting seems needed
                if any(word in subquery.lower() for word in ['order', 'sort', 'highest', 'lowest', 'top', 'bottom']):
                    order_cols = columns[:1] if columns else []
                    if order_cols:
                        direction = "DESC" if any(word in subquery.lower() for word in ['highest', 'top', 'most']) else "ASC"
                        order_sql = sql + f" ORDER BY {order_cols[0]} {direction}"
                        sql_candidates.append(order_sql)
                        
                # Version with LIMIT if bounds seem needed
                if any(word in subquery.lower() for word in ['top', 'first', 'limit', 'recent']):
                    limit_sql = sql + " LIMIT 10"
                    sql_candidates.append(limit_sql)
        
        # Ensure we return at most 'limit' SQL statements
        return sql_candidates[:limit]
    
    def _structure_paths(self, paths):
        """
        Convert raw paths into structured representations for SQL generation.
        
        Args:
            paths: List of path strings
            
        Returns:
            List of structured path dictionaries
        """
        structured = []
        
        for path in paths:
            elements = path.split('.')
            if len(elements) < 2:
                continue
                
            # Determine the main table and columns
            tables = [elem for elem in elements if '.' not in elem]
            if not tables:
                continue
                
            main_table = tables[0]
            columns = []
            joins = []
            
            # Extract columns and joins
            for i, elem in enumerate(elements):
                if '.' in elem:
                    # This is a column
                    table, column = elem.split('.')
                    columns.append(f"{table}.{column}")
                    
                    # If this is a column from a different table, add a join
                    if table != main_table and table in tables:
                        # Find a potential join path
                        for j in range(i):
                            if elements[j] == table or (j+1 < len(elements) and elements[j+1] == table):
                                # Found a connection to this table
                                if '.' in elements[j]:
                                    join_col = elements[j]
                                    join_table, join_column = join_col.split('.')
                                    
                                    # Add a join if not already present
                                    if not any(j['table'] == table for j in joins):
                                        joins.append({
                                            'table': table,
                                            'condition': f"{main_table}.{join_column} = {table}.{join_column}"
                                        })
            
            # If no columns were found, use * as a fallback
            if not columns:
                columns = [f"{main_table}.*"]
            
            structured.append({
                'main_table': main_table,
                'columns': columns,
                'joins': joins
            })
        
        return structured
    
    def _generate_where_clause(self, subquery, struct):
        """
        Generate a WHERE clause based on the subquery and structure.
        
        Args:
            subquery: The subquery text
            struct: Structured path information
            
        Returns:
            WHERE clause string or empty string
        """
        # Simple approach: extract potential filter conditions from the query
        where_clauses = []
        
        # Look for comparison patterns in the subquery
        comparisons = ['greater than', 'less than', 'equal to', 'more than', 
                      'at least', 'at most', 'between', 'like']
        
        # Check for numbers in the query
        numbers = re.findall(r'\b\d+\b', subquery)
        date_keywords = ['date', 'year', 'month', 'today', 'yesterday', 'current']
        
        columns = struct.get('columns', [])
        
        # If we found numbers and have columns
        if numbers and columns:
            for number in numbers:
                # Find potential comparison operators
                if any(comp in subquery for comp in ['greater', 'more', 'over', 'above', 'exceeds']):
                    op = '>'
                elif any(comp in subquery for comp in ['less', 'under', 'below']):
                    op = '<'
                else:
                    op = '='
                
                # Select an appropriate column
                for col in columns:
                    col_name = col.split('.')[-1] if '.' in col else col
                    if any(keyword in col_name.lower() for keyword in ['id', 'count', 'num', 'total']):
                        where_clauses.append(f"{col} {op} {number}")
                        break
                else:
                    # Default to the first column if no suitable column found
                    if columns:
                        where_clauses.append(f"{columns[0]} {op} {number}")
        
        # Check for date filters
        if any(keyword in subquery.lower() for keyword in date_keywords):
            for col in columns:
                col_name = col.split('.')[-1] if '.' in col else col
                if any(keyword in col_name.lower() for keyword in ['date', 'time', 'year', 'created', 'updated']):
                    if 'today' in subquery.lower():
                        where_clauses.append(f"{col} = date('now')")
                    elif 'yesterday' in subquery.lower():
                        where_clauses.append(f"{col} = date('now', '-1 day')")
                    elif 'this month' in subquery.lower():
                        where_clauses.append(f"strftime('%Y-%m', {col}) = strftime('%Y-%m', 'now')")
                    elif 'this year' in subquery.lower():
                        where_clauses.append(f"strftime('%Y', {col}) = strftime('%Y', 'now')")
                    break
        
        # Look for named entities (people, products, etc.)
        name_pattern = r'"([^"]+)"'
        names = re.findall(name_pattern, subquery)
        if names:
            # For each potential name/string value
            for name in names:
                # Find a suitable column
                for col in columns:
                    col_name = col.split('.')[-1] if '.' in col else col
                    if any(keyword in col_name.lower() for keyword in ['name', 'title', 'description']):
                        where_clauses.append(f"{col} LIKE '%{name}%'")
                        break
        
        # Combine all clauses with AND
        if where_clauses:
            return ' AND '.join(where_clauses)
        return ''


class OptimizedQueryProcessor:
    """
    Main class for processing natural language queries using the graph-based approach.
    """
    
    def __init__(self, db_file):
        """
        Initialize the query processor with a database file.
        
        Args:
            db_file: Path to the SQLite database file
        """
        # Create schema graph
        self.graph, self.metadata = create_schema_graph(db_file)
        
        # Initialize components
        self.path_finder = OptimizedWeightedFlowPathFinder()
        self.sql_generator = OptimizedSQLGenerator(self.metadata)
        
        # Store database file for later use
        self.db_file = db_file
    
    def decompose_query(self, query):
        """
        Decompose a complex query into simpler subqueries.
        
        Args:
            query: The original natural language question
            
        Returns:
            List of subqueries
        """
        # Check if it's already a simple query
        word_count = len(query.split())
        if word_count <= 10:
            return [query]
        
        # Simple rule-based decomposition
        subqueries = []
        
        # Split on connectors like "and", "or", "as well as"
        if " and " in query.lower():
            parts = re.split(r'\band\b', query, flags=re.IGNORECASE)
            for part in parts:
                part = part.strip()
                if part and len(part.split()) >= 3:  # Ensure it's substantial
                    subqueries.append(part)
        elif " or " in query.lower():
            parts = re.split(r'\bor\b', query, flags=re.IGNORECASE)
            for part in parts:
                part = part.strip()
                if part and len(part.split()) >= 3:  # Ensure it's substantial
                    subqueries.append(part)
        # Split on question patterns
        elif "?" in query:
            parts = query.split("?")
            for part in parts:
                part = part.strip()
                if part and len(part.split()) >= 3:  # Ensure it's substantial
                    subqueries.append(part + "?")
        
        # If no clear decomposition, look for components
        if not subqueries:
            # Look for patterns like "Find X that Y"
            match = re.search(r'\b(find|show|list|get|retrieve)\b(.*?)\b(that|which|where|when)\b(.*)', 
                             query, re.IGNORECASE)
            if match:
                main_part = match.group(1) + match.group(2)
                condition_part = match.group(3) + match.group(4)
                subqueries = [main_part.strip(), condition_part.strip()]
            # Look for various SQL-like components
            elif any(keyword in query.lower() for keyword in ['order by', 'group by', 'sorted']):
                # Split query into main part and sorting/grouping part
                for keyword in ['order by', 'group by', 'sorted by']:
                    if keyword in query.lower():
                        parts = re.split(rf'\b{re.escape(keyword)}\b', query, flags=re.IGNORECASE)
                        if len(parts) == 2:
                            subqueries = [parts[0].strip(), f"{keyword} {parts[1].strip()}"]
                            break
        
        # If still no decomposition, try to identify main entities and actions
        if not subqueries:
            # Identify main entities
            tables = self.metadata.get('tables', [])
            entity_matches = []
            
            for table in tables:
                if table.lower() in query.lower():
                    entity_matches.append(table)
            
            # If we found multiple entities, decompose based on them
            if len(entity_matches) > 1:
                for entity in entity_matches:
                    # Find sentences or clauses that mention this entity
                    pattern = fr"[^.!?]*\b{re.escape(entity)}\b[^.!?]*[.!?]?"
                    matches = re.findall(pattern, query, re.IGNORECASE)
                    for match in matches:
                        if match.strip() and len(match.strip().split()) >= 3:
                            subqueries.append(match.strip())
        
        # If we still couldn't decompose, use the original query
        if not subqueries:
            subqueries = [query]
        
        # Deduplicate and ensure we don't have too many subqueries
        subqueries = list(dict.fromkeys(subqueries))  # Remove duplicates while preserving order
        
        # Limit to at most 3 subqueries
        if len(subqueries) > 3:
            # Keep the most diverse set of subqueries
            subqueries = subqueries[:3]
