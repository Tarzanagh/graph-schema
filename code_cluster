import json
import re
import requests
import sqlparse
import difflib
import os
import sys
import argparse
import math
import random

class GraphBasedQueryProcessor:
    """Main class for query processing pipeline working directly with a graph structure."""
    
    def __init__(self, api_url, api_key, model):
        """
        Initialize the query processor.
        
        Args:
            api_url: URL for the LLM API
            api_key: API key for authentication
            model: Model name to use
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
        
        # Initialize components for each stage of the pipeline
        self.query_decomposer = QueryDecomposer(api_url, api_key, model)
        self.path_finder = GraphBasedPathFinder(api_url, api_key, model)
        self.sql_generator = SQLGenerator(api_url, api_key, model)
    
    def process_query(self, graph, question):
        """
        Process a natural language query through the pipeline:
        1. Decompose into subqueries
        2. Find top 10 relevant paths for each subquery using weighted flow diffusion on the graph
        3. Generate top 10 SQL for each subquery based on paths with rewards
        
        Args:
            graph: A graph structure containing nodes and edges
            question: Natural language question
            
        Returns:
            Dictionary with detailed processing results
        """
        # Step 1: Decompose the query into subqueries
        subqueries = self.query_decomposer.decompose_query(question)
        
        # Step 2 & 3: Process each subquery to find paths and generate SQL
        subquery_results = []
        
        for i, subquery in enumerate(subqueries):
            # Find top 10 relevant paths for this subquery with relevance scores using weighted flow diffusion
            path_results = self.path_finder.find_top_relevant_paths(graph, subquery, limit=10)
            
            # Extract paths from the graph for SQL generation
            paths_data = self._extract_paths_from_graph(graph, path_results["paths"])
            
            # Generate top 10 SQL for this subquery using the found paths
            sql_results = self.sql_generator.generate_top_subsqls(paths_data, subquery, path_results["paths"], limit=10)
            
            # Combine results for this subquery
            subquery_results.append({
                "subquery_id": f"Subquery {chr(65 + i)}",  # A, B, C, etc.
                "subquery_text": subquery,
                "top_paths": path_results["paths"],
                "path_relevance_scores": path_results["relevance_scores"],
                "top_sqls": sql_results["sqls"],
                "sql_rewards": sql_results["rewards"]
            })
        
        # Return the processing results
        return {
            "original_query": question,
            "subqueries": subqueries,
            "subquery_results": subquery_results
        }
    
    def _extract_paths_from_graph(self, graph, relevant_paths):
        """
        Convert relevant paths from the graph structure to the format expected by SQLGenerator.
        
        Args:
            graph: The graph structure
            relevant_paths: List of path identifiers
            
        Returns:
            List of path strings formatted as needed by SQLGenerator
        """
        # For the paths that are already in string format, we can use them directly
        # For paths that are in path ID format, we need to convert them to actual path strings
        paths_data = []
        
        for path in relevant_paths:
            if isinstance(path, str) and '.' in path:
                # This is already in the expected format (e.g., "table1.field1.table2.field2")
                paths_data.append(path)
            else:
                # This is a path ID or node sequence we need to convert
                # This implementation depends on your graph structure
                if isinstance(path, (list, tuple)):
                    # If the path is a list of nodes, join them with dots
                    paths_data.append('.'.join(path))
                else:
                    # Try to look up the path in the graph's paths dictionary if it exists
                    if hasattr(graph, 'paths') and path in graph.paths:
                        paths_data.append(graph.paths[path])
                    else:
                        # Default fallback - use the path ID as is
                        paths_data.append(str(path))
        
        return paths_data


class QueryDecomposer:
    """Component for decomposing a natural language query into subqueries."""
    
    def __init__(self, api_url, api_key, model):
        """Initialize the query decomposer."""
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
    
    def decompose_query(self, question):
        """
        Decompose a complex query into simpler subqueries.
        
        Args:
            question: The original natural language question
            
        Returns:
            List of subqueries
        """
        prompt = f"""
        Please decompose the following complex query into simpler subqueries:
        
        QUERY: {question}
        
        Break this down into 2-4 distinct subqueries, where each subquery addresses a specific part of the overall question.
        Format your response as:
        <SUBQUERIES>
        1. First subquery text
        2. Second subquery text
        ...
        </SUBQUERIES>
        """
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are an expert at breaking down complex queries into simpler components."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=data)
            response.raise_for_status()
            response_data = response.json()
            
            # Extract the generated subqueries from the response
            content = response_data["choices"][0]["message"]["content"]
            
            # Extract subqueries from the tags
            subqueries_match = re.search(r'<SUBQUERIES>(.*?)</SUBQUERIES>', content, re.DOTALL)
            
            if subqueries_match:
                subqueries_text = subqueries_match.group(1).strip()
                # Extract numbered items, removing the numbers
                subquery_list = re.findall(r'^\d+\.\s*(.*?)$', subqueries_text, re.MULTILINE)
                
                if subquery_list:
                    return subquery_list
            
            # Fallback: if parsing fails, return the original query as a single subquery
            return [question]
                
        except Exception as e:
            print(f"Error decomposing query: {e}")
            # Fallback to a basic decomposition
            return [question]


class GraphBasedPathFinder:
    """
    Component for finding relevant paths in a graph using Weighted Flow Diffusion algorithm.
    Uses both graph structure and node attributes to find local clusters (paths).
    """
    
    def __init__(self, api_url=None, api_key=None, model=None):
        """Initialize the graph-based path finder."""
        self.api_url = api_url  # Used for attribute extraction if needed
        self.api_key = api_key  # Used for attribute extraction if needed
        self.model = model      # Used for attribute extraction if needed
        self.gamma = 0.02       # Parameter for Gaussian kernel weighting
        self.max_iterations = 30  # Maximum iterations for flow diffusion
    
    def extract_attributes(self, graph, query):
        """
        Extract attributes for nodes based on semantic similarity to query.
        
        Args:
            graph: The graph representation
            query: The query text to compare against
            
        Returns:
            Updated graph with node attributes
        """
        query_terms = set(query.lower().split())
        
        for node_id, node_data in graph['nodes'].items():
            # Simple attribute: how many query terms appear in the node name
            node_terms = set(node_id.lower().split('_'))
            overlap = len(query_terms.intersection(node_terms))
            
            # Store as attribute
            node_data['query_relevance'] = overlap
        
        return graph
    
    def reweight_edges(self, graph):
        """
        Reweight edges based on attribute similarity using Gaussian kernel.
        
        Args:
            graph: The graph with node attributes
            
        Returns:
            Updated graph with reweighted edges
        """
        # Reweight each edge
        for i, (source, target, _) in enumerate(graph['edges']):
            source_attrs = graph['nodes'][source]
            target_attrs = graph['nodes'][target]
            
            # Calculate attribute distance (based on query_relevance)
            # Higher distance = less similar
            dist = abs(source_attrs.get('query_relevance', 0) - target_attrs.get('query_relevance', 0))
            
            # Apply Gaussian kernel: weight = exp(-gamma * distÂ²)
            weight = math.exp(-self.gamma * (dist ** 2))
            
            # Update edge weight
            graph['edges'][i] = (source, target, weight)
        
        return graph
    
    def flow_diffusion(self, graph, seed_node, alpha=5.0):
        """
        Run the flow diffusion algorithm to find relevant nodes.
        
        Args:
            graph: The graph representation
            seed_node: The starting node for diffusion
            alpha: Source mass multiplier
            
        Returns:
            Dictionary mapping nodes to their importance scores
        """
        # Initialize node embeddings and mass
        x = {node_id: 0.0 for node_id in graph['nodes']}
        m = {node_id: 0.0 for node_id in graph['nodes']}
        
        # Set source mass
        if seed_node in m:
            m[seed_node] = alpha
        else:
            # If seed node not found, find best match
            best_match = None
            best_score = -1
            
            for node_id in graph['nodes']:
                if seed_node.lower() in node_id.lower():
                    relevance = graph['nodes'][node_id].get('query_relevance', 0)
                    if relevance > best_score:
                        best_match = node_id
                        best_score = relevance
            
            if best_match:
                m[best_match] = alpha
            else:
                # Fall back to a node with highest query relevance
                scores = [(node_id, data.get('query_relevance', 0)) 
                          for node_id, data in graph['nodes'].items()]
                best_node = max(scores, key=lambda x: x[1])[0]
                m[best_node] = alpha
        
        # Set sink capacities (here using uniform capacity of 1.0)
        t = {node_id: 1.0 for node_id in graph['nodes']}
        
        # Run flow diffusion for specified iterations
        for _ in range(self.max_iterations):
            # Find nodes with excess mass
            overflow_nodes = [node for node, mass in m.items() if mass > t[node]]
            
            if not overflow_nodes:
                break
                
            # Pick a random overflow node
            i = random.choice(overflow_nodes)
            
            # Calculate weighted degree
            w_i = sum(w for s, t, w in graph['edges'] if s == i or t == i)
            
            if w_i == 0:
                continue  # Skip isolated nodes
                
            # Update node embedding
            x[i] += (m[i] - t[i]) / w_i
            
            # Set excess mass to distribute
            excess = m[i] - t[i]
            m[i] = t[i]
            
            # Distribute excess mass to neighbors
            for j in graph['adjacency'][i]:
                # Find edge weight
                edge_weight = next((w for s, t, w in graph['edges'] 
                                  if (s == i and t == j) or (s == j and t == i)), 1.0)
                
                # Update mass at neighbor
                m[j] += excess * edge_weight / w_i
        
        return x
    
    def identify_seed_node(self, graph, query):
        """
        Identify the best seed node based on query relevance.
        
        Args:
            graph: The graph representation
            query: The query text
            
        Returns:
            Best seed node
        """
        best_node = None
        best_score = -1
        
        for node_id, data in graph['nodes'].items():
            score = data.get('query_relevance', 0)
            if score > best_score:
                best_node = node_id
                best_score = score
        
        return best_node
    
    def extract_paths_from_subgraph(self, graph, important_nodes, limit=10):
        """
        Extract paths from the subgraph of important nodes identified by flow diffusion.
        
        Args:
            graph: The graph representation
            important_nodes: Dictionary mapping nodes to importance scores
            limit: Maximum number of paths to return
            
        Returns:
            Dictionary with paths and their relevance scores
        """
        # Filter nodes with positive importance
        active_nodes = {node: score for node, score in important_nodes.items() if score > 0}
        
        if not active_nodes:
            return {"paths": [], "relevance_scores": {}}
        
        # Find paths by doing a breadth-first search from each important node
        # This is a simplified approach - you might want to use a more sophisticated path finding algorithm
        paths = []
        path_scores = {}
        
        # Sort nodes by importance for prioritization
        sorted_nodes = sorted(active_nodes.items(), key=lambda x: x[1], reverse=True)
        
        # For each important node, find paths to other important nodes
        for start_node, start_score in sorted_nodes[:min(5, len(sorted_nodes))]:
            # BFS to find paths
            queue = [(start_node, [start_node])]
            visited = set([start_node])
            
            while queue and len(paths) < limit * 2:  # Find more paths than needed, then filter
                current_node, current_path = queue.pop(0)
                
                # If we've reached another important node, record the path
                if current_node != start_node and current_node in active_nodes:
                    # Convert path to string format
                    path_str = '.'.join(current_path)
                    
                    # Calculate path score as sum of node importance scores
                    score = sum(active_nodes.get(node, 0) for node in current_path)
                    
                    paths.append(path_str)
                    path_scores[path_str] = score
                
                # Continue BFS if path is not too long
                if len(current_path) < 5:  # Limit path length to avoid exponential explosion
                    for neighbor in graph['adjacency'].get(current_node, []):
                        if neighbor not in visited:
                            visited.add(neighbor)
                            queue.append((neighbor, current_path + [neighbor]))
        
        # Sort paths by score and take top limit
        sorted_paths = sorted(path_scores.items(), key=lambda x: x[1], reverse=True)
        top_paths = sorted_paths[:limit]
        
        # Normalize scores to [0, 1] range
        max_score = max([score for _, score in top_paths]) if top_paths else 1.0
        normalized_scores = {path: score/max_score for path, score in top_paths}
        
        # Generate result format
        paths = [path for path, _ in top_paths]
        
        # If fewer than limit paths found, add placeholders
        if len(paths) < limit:
            for i in range(len(paths), limit):
                placeholder = f"path_{i+1}"
                paths.append(placeholder)
                normalized_scores[placeholder] = max(0.0, 1.0 - (i * 0.1))
        
        return {
            "paths": paths,
            "relevance_scores": normalized_scores
        }
    
    def find_top_relevant_paths(self, graph, subquery, limit=10, seed_node=None):
        """
        Find top relevant paths for a given subquery using weighted flow diffusion.
        
        Args:
            graph: The graph structure containing nodes and edges
            subquery: The subquery text
            limit: Maximum number of paths to return (default: 10)
            seed_node: Optional seed node to start from
            
        Returns:
            Dictionary with paths and their relevance scores
        """
        try:
            # Step 1: Extract node attributes based on query relevance
            graph_with_attributes = self.extract_attributes(graph, subquery)
            
            # Step 2: Reweight edges based on attribute similarity
            graph_with_weights = self.reweight_edges(graph_with_attributes)
            
            # Step 3: Identify best seed node if not provided
            if not seed_node:
                seed_node = self.identify_seed_node(graph_with_weights, subquery)
            
            # Step 4: Run flow diffusion from seed node
            node_importance = self.flow_diffusion(graph_with_weights, seed_node)
            
            # Step 5: Extract relevant paths based on node importance
            result = self.extract_paths_from_subgraph(graph_with_weights, node_importance, limit)
            
            return result
            
        except Exception as e:
            print(f"Error in weighted flow diffusion: {e}")
            # Return placeholders on error
            placeholders = {}
            placeholder_paths = []
            
            for i in range(limit):
                path = f"error_path_{i+1}"
                placeholder_paths.append(path)
                placeholders[path] = max(0.1, 1.0 - (i * 0.1))
            
            return {
                "paths": placeholder_paths,
                "relevance_scores": placeholders
            }


class SQLGenerator:
    """Component for generating SQL for subqueries based on relevant paths."""
    
    def __init__(self, api_url, api_key, model):
        """Initialize the SQL generator."""
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
    
    def generate_top_subsqls(self, paths_data, subquery, relevant_paths, limit=10):
        """
        Generate top SQL statements for a subquery based on relevant paths and automatically evaluate them.
        
        Args:
            paths_data: List of path strings
            subquery: The subquery text
            relevant_paths: List of relevant paths for this subquery
            limit: Maximum number of SQL statements to generate (default: 10)
            
        Returns:
            Dictionary with generated SQLs and their computed rewards
        """
        # Convert paths data and relevant paths to strings for the prompt
        paths_str = json.dumps(paths_data, indent=2)
        relevant_paths_str = json.dumps(relevant_paths, indent=2)
        
        prompt = f"""
        Generate {limit} different SQL statements for the following subquery using the given relevant paths.
        
        AVAILABLE PATHS:
        {paths_str}
        
        RELEVANT PATHS FOR THIS SUBQUERY:
        {relevant_paths_str}
        
        SUBQUERY: {subquery}
        
        Generate {limit} different, valid SQL statements that correctly answer this subquery using the provided paths.
        Each SQL should be complete and executable. Ensure variety in the solutions.
        
        Format your response as:
        <SQL_1>
        Your first SQL code here
        </SQL_1>
        
        <SQL_2>
        Your second SQL code here
        </SQL_2>
        
        ... and so on for all {limit} SQL statements
        """
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are an expert SQL generator."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.5,  # Slightly higher temperature for more variety
            "max_tokens": 4000
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=data)
            response.raise_for_status()
            response_data = response.json()
            
            # Extract the generated SQLs from the response
            content = response_data["choices"][0]["message"]["content"]
            
            sqls = []
            
            # Extract all SQL statements
            for i in range(1, limit + 1):
                # Extract SQL from the tags
                sql_match = re.search(rf'<SQL_{i}>(.*?)</SQL_{i}>', content, re.DOTALL)
                if sql_match:
                    sql = sql_match.group(1).strip()
                    sqls.append(sql)
            
            # If fewer than limit SQLs were generated, add placeholders
            if len(sqls) < limit:
                for i in range(len(sqls), limit):
                    placeholder_sql = f"-- Placeholder SQL #{i+1} for: {subquery}\nSELECT * FROM table WHERE condition_{i+1};"
                    sqls.append(placeholder_sql)
            
            # Now evaluate each SQL statement
            rewards = self._evaluate_sql_statements(sqls, subquery, paths_data, relevant_paths)
            
            return {
                "sqls": sqls,
                "rewards": rewards
            }
                
        except Exception as e:
            print(f"Error generating SQLs: {e}")
            # Return placeholders on error
            placeholder_sqls = []
            placeholder_rewards = {}
            
            for i in range(limit):
                sql = f"-- Error generating SQL #{i+1} for: {subquery}\nSELECT * FROM error_table WHERE error_condition_{i+1};"
                placeholder_sqls.append(sql)
                
                placeholder_rewards[sql] = {
                    "exec_correctness": 0.0,  # Cannot execute
                    "schema_linking": 0.0,
                    "dialect_function": 0.0,
                    "query_planning": 0.0,
                    "efficiency": 0.0,
                    "divergence": 0.0
                }
                
            return {
                "sqls": placeholder_sqls,
                "rewards": placeholder_rewards
            }
    
    def _evaluate_sql_statements(self, sql_statements, subquery, paths_data, relevant_paths):
        """
        Evaluate SQL statements based on execution and other metrics.
        
        Args:
            sql_statements: List of SQL statements to evaluate
            subquery: The original subquery
            paths_data: JSON data containing available paths
            relevant_paths: List of relevant paths for this subquery
            
        Returns:
            Dictionary mapping each SQL statement to its reward metrics
        """
        rewards = {}
        
        for sql in sql_statements:
            # Initialize rewards structure
            sql_rewards = {
                "exec_correctness": 0.0,
                "schema_linking": 0.0,
                "dialect_function": 0.0,
                "query_planning": 0.0,
                "efficiency": 0.0,
                "divergence": 0.0
            }
            
            # Test 1: Check if the SQL is valid syntax
            try:
                # Try to parse the SQL using sqlparse
                parsed = sqlparse.parse(sql)
                if parsed:
                    # If it parses successfully, it has at least valid syntax
                    sql_rewards["exec_correctness"] = 0.5  # Base score for valid syntax
                else:
                    sql_rewards["exec_correctness"] = 0.0
            except Exception:
                sql_rewards["exec_correctness"] = 0.0
            
            # Test 2: Check for correct schema linking
            sql_rewards["schema_linking"] = self._evaluate_schema_linking(sql, paths_data, relevant_paths)
            
            # Test 3: Check for dialect functions
            sql_rewards["dialect_function"] = self._evaluate_dialect_functions(sql)
            
            # Test 4: Evaluate query planning
            sql_rewards["query_planning"] = self._evaluate_query_planning(sql)
            
            # Test 5: Evaluate efficiency
            sql_rewards["efficiency"] = self._evaluate_efficiency(sql)
            
            # Test 6: Evaluate divergence (compare to other generated SQLs)
            sql_rewards["divergence"] = self._evaluate_divergence(sql, sql_statements)
            
            # Store the rewards for this SQL
            rewards[sql] = sql_rewards
        
        return rewards
    
    def _evaluate_schema_linking(self, sql, paths_data, relevant_paths):
        """Evaluate how well the SQL uses proper schema elements from paths"""
        # Extract tables and columns from SQL
        tables_pattern = r'FROM\s+([a-zA-Z0-9_]+)'
        columns_pattern = r'SELECT\s+(.*?)\s+FROM'
        
        tables_match = re.search(tables_pattern, sql, re.IGNORECASE)
        columns_match = re.search(columns_pattern, sql, re.IGNORECASE | re.DOTALL)
        
        # If no tables or columns found, give a low score
        if not tables_match or not columns_match:
            return 0.1
        
        # Check if tables mentioned in SQL appear in the relevant paths
        tables = tables_match.group(1).split(',')
        tables = [t.strip() for t in tables]
        
        # Check if tables are in the relevant paths
        table_match_score = 0.0
        for table in tables:
            if any(table in path for path in relevant_paths):
                table_match_score += 1.0
        
        if tables:
            table_match_score /= len(tables)
        
        # Extract columns from the SELECT clause
        columns_text = columns_match.group(1)
        # Handle wildcard case
        if '*' in columns_text:
            column_match_score = 0.5  # Wildcard gets a medium score
        else:
            columns = columns_text.split(',')
            columns = [c.strip() for c in columns]
            
            # Check if columns are likely to be correct based on paths
            column_match_score = 0.0
            for column in columns:
                # Remove table prefix if present
                if '.' in column:
                    column = column.split('.')[1]
                
                # Check if column appears in paths
                if any(column in path for path in relevant_paths):
                    column_match_score += 1.0
            
            if columns:
                column_match_score /= len(columns)
        
        # Evaluate join conditions
        join_score = 0.0
        if 'JOIN' in sql.upper():
            # Simple heuristic: joins exist and seem to connect relevant tables
            join_score = 0.8
            
            # Check for ON conditions which suggest proper joins
            if 'ON' in sql.upper() and '=' in sql:
                join_score = 1.0
        
        # Combine scores, weighted toward tables and joins
        final_score = (table_match_score * 0.4) + (column_match_score * 0.3) + (join_score * 0.3)
        return final_score
    
    def _evaluate_dialect_functions(self, sql):
        """Evaluate correct usage of dialect-specific functions"""
        sql_upper = sql.upper()
        
        # List of common SQL dialect functions
        dialect_functions = [
            'ST_DISTANCE', 'ST_CONTAINS', 'ST_WITHIN',  # Spatial
            'DATE_TRUNC', 'TO_TIMESTAMP', 'EXTRACT',    # Date/time
            'REGEXP_MATCHES', 'REGEXP_REPLACE',         # Regex
            'ARRAY_AGG', 'UNNEST', 'JSON_EXTRACT',      # Arrays/JSON
            'PERCENTILE_CONT', 'MEDIAN', 'STDDEV',      # Statistical
            'RANK', 'DENSE_RANK', 'ROW_NUMBER'          # Window functions
        ]
        
        function_count = 0
        for func in dialect_functions:
            if func in sql_upper:
                function_count += 1
        
        # Check for proper syntax around functions
        syntax_pattern = r'(\w+)\s*\([\w\s,*]+\)'
        syntax_matches = re.findall(syntax_pattern, sql)
        syntax_score = min(1.0, len(syntax_matches) * 0.2)  # Scale based on function count
        
        # If no functions are used, assign a neutral score
        if function_count == 0:
            return 0.5
        
        # Score based on proper function usage and syntax
        return min(1.0, 0.3 + (function_count * 0.15) + (syntax_score * 0.55))
    
    def _evaluate_query_planning(self, sql):
        """Evaluate multi-step or nested SQL structures"""
        # ... [implementation remains the same as in original code] ...
        sql_upper = sql.upper()
        
        # Check for advanced SQL structures
        has_cte = 'WITH' in sql_upper
        has_subquery = '(' in sql and 'SELECT' in sql_upper[sql_upper.find('('):] if '(' in sql_upper else False
        has_union = 'UNION' in sql_upper
        has_group_by = 'GROUP BY' in sql_upper
        has_having = 'HAVING' in sql_upper
        has_order_by = 'ORDER BY' in sql_upper
        has_limit = 'LIMIT' in sql_upper
        
        # Count the structural elements
        structure_count = sum([has_cte, has_subquery, has_union, has_group_by, has_having, has_order_by, has_limit])
        
        # Check for nested subqueries (more complex planning)
        nesting_level = 0
        inside_subquery = False
        for char in sql:
            if char == '(':
                if inside_subquery:
                    nesting_level += 1
                elif 'SELECT' in sql_upper[sql.find('(') - 10:sql.find('(')]:
                    inside_subquery = True
            elif char == ')' and inside_subquery:
                if nesting_level > 0:
                    nesting_level -= 1
                else:
                    inside_subquery = False
        
        # More complex nesting gets higher scores
        nesting_score = min(1.0, nesting_level * 0.2)
        
        # Simple scoring based on complexity
        base_score = 0.3
        complexity_score = min(0.7, structure_count * 0.1)
        
        return base_score + complexity_score + nesting_score * 0.3
    
    def _evaluate_efficiency(self, sql):
        """Evaluate SQL for efficiency based on static analysis"""
        sql_upper = sql.upper()
        
        # Check for common inefficient patterns
        has_select_star = 'SELECT *' in sql_upper
        has_cartesian_join = 'FROM' in sql_upper and 'JOIN' in sql_upper and 'ON' not in sql_upper
        has_distinct = 'DISTINCT' in sql_upper
        has_or_conditions = ' OR ' in sql_upper
        has_not_in = 'NOT IN' in sql_upper
        has_subquery_in_where = 'WHERE' in sql_upper and '(' in sql_upper and 'SELECT' in sql_upper[sql_upper.find('WHERE'):] 
        
        # Count potential inefficiencies
        inefficiencies = sum([has_select_star, has_cartesian_join, has_distinct, has_or_conditions, has_not_in, has_subquery_in_where])
        
        # Check for efficiency optimizations
        has_index_hint = 'INDEX' in sql_upper
        has_limit = 'LIMIT' in sql_upper
        has_specific_columns = not has_select_star
        has_proper_joins = 'JOIN' in sql_upper and 'ON' in sql_upper
        
        # Count optimizations
        optimizations = sum([has_index_hint, has_limit, has_specific_columns, has_proper_joins])
        
        # Calculate efficiency score
        base_score = 0.5
        inefficiency_penalty = min(0.5, inefficiencies * 0.1)
        optimization_bonus = min(0.5, optimizations * 0.125)
        
        return max(0.0, min(1.0, base_score - inefficiency_penalty + optimization_bonus))
